RNN：
	使用了“序列”的信息，就是让信息有前后的关联而不是独立考虑。
	即，前面的输出也作为后面的输入这样吧。
	重复模块具有的结构比较简单，比如单个tanh层。
RNN缺点：
	但是实际使用中RNN很容易陷入梯度消失和梯度爆炸，
	所以不能很好的联系距离较长的前后文。
LSTM：
	克服RNN的长期记忆不足而生。
	在每一个重复模块中具有更复杂的结构。
	每个LSTM元胞中有四种不同的网络层。
	是图中四个remember、save、focus、ltm吗？
Sigmoid函数：
	它它它原来就是S型生长曲线啊。 
	特点是单增以及反函数单增。
	常被用作神经网络的阈值函数，将变量映射到0,1之间。
Softmax函数：
	作为激活函数。
	该函数的特点是可以将输出值转换为 0-1 之间的一个小数值，并且这些小数值的和为 1。
	原来如此啊……难怪之前好多都要softmax层……
